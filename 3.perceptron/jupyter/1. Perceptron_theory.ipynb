{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机是二分类的线性分类模型，本质上想找到一条直线或者分离超平面对数据进行线性划分\n",
    "- 适用于线性可分的数据集，否则感知机不会收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有一个数据集$D = {(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)}$，其中$x_i \\in R^n$，即$x_i = (x_i^{(1)}, x_i^{(2)}, ...x_i^{(n)})$\n",
    "- 模型的输入为实例的特征向量$x_i$，输出为实例的类别，取值为+1（正例）或者-1（负例）\n",
    "- 我们希望找到一个分离超平面$w^Tx + b = 0，其中w \\in R^n$，使得有满足$w^Tx + b > 0$的实例所对应的类别为正例。而使得满足$w^Tx + b < 0$的实例所对应的类别为负例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是我们可以构建出感知机模型为：$f(x) = sign(w^Tx + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数一个很自然的想法是建立在误分类点的个数上，但是使用误分类点的个数来构造损失函数并不容易优化\n",
    "- 因此使用**误分类点到分离超平面的总距离**来构造损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T07:24:32.171940Z",
     "start_time": "2020-07-19T07:24:32.168367Z"
    }
   },
   "source": [
    "记M为误分类点的集合，误分类点到分离超平面的总距离为：\n",
    "$$L(w, b) = \\sum_{x_i \\in M}\\frac{| w^Tx_i + b |}{\\parallel w \\parallel}$$\n",
    "\n",
    "不考虑$\\frac{1}{\\parallel w \\parallel}$（因为上式中，分子和分母有固定倍数的关系），并且去掉绝对值，就可以得到感知机的损失函数为：\n",
    "$$L(w, b) = \\sum_{x_i \\in M} -y_i (w^Tx_i + b)$$\n",
    "此时对于误分类点，$-y_i (w^Tx_i + b) > 0$成立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T07:42:59.061565Z",
     "start_time": "2020-07-19T07:42:59.056019Z"
    }
   },
   "source": [
    "此时感知机算法就转变为，求解参数$w, b$，使得损失函数极小化，即\n",
    "$$\\underset {w, b}{\\arg \\min L(w, b)} = \\underset {w, b}{\\arg \\min} \\sum_{x_i \\in M} -y_i (w^Tx_i + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T07:37:21.005010Z",
     "start_time": "2020-07-19T07:37:21.000136Z"
    }
   },
   "source": [
    "因为只有对误分类点才会对损失函数进行优化，因此感知机的优化采用随机梯度下降法（SGD），而非使用所有样本的批量随机梯度下降法（BGD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T07:48:03.056393Z",
     "start_time": "2020-07-19T07:48:03.052579Z"
    }
   },
   "source": [
    "损失函数$L(w, b)$的梯度为：\n",
    "$$\\frac{\\partial L(w, b)}{\\partial w} = -\\sum_{x_i \\in M} y_i x_i$$\n",
    "$$\\frac{\\partial L(w, b)}{\\partial b} = -\\sum_{x_i \\in M} y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T07:53:23.210553Z",
     "start_time": "2020-07-19T07:53:23.205152Z"
    }
   },
   "source": [
    "对于SGD，选取一个误分类点进行更新，即有：\n",
    "$$w_{t+1} = w_t + \\alpha y_ix_i$$\n",
    "$$b_{t+1} = b_t + \\alpha y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机的原始算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T07:56:09.602832Z",
     "start_time": "2020-07-19T07:56:09.597425Z"
    }
   },
   "source": [
    "训练集包括N个样例，样本中包含n个特征，标记为二分类取值为-1或者+1\n",
    "- 输入的样例：${(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)}$，学习率：$\\alpha$\n",
    "- 输出分离超平面的系数w, b\n",
    "\n",
    "算法执行步骤如下：\n",
    "1. 初始化w, b以及学习率$\\alpha$\n",
    "2. 在训练集选取数据$(x_i, y_i)$\n",
    "3. 如果满足$-y_i(sign(w^Tx_i + b)) > 0$，则\n",
    "$$w_{t+1} = w_t + \\alpha y_i x_i$$\n",
    "$$b_{t+1} = b_t + \\alpha y_i$$\n",
    "4. 转至2，直到训练集中没有误分类点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机的对偶算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对偶形式的基本想法是，将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解它的系数来求解$w$和$b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T08:40:40.748657Z",
     "start_time": "2020-07-21T08:40:40.741727Z"
    }
   },
   "source": [
    "假设初始值$w_0$和$b_0$都为0，因此$w$和$b$可以表示成$x_iy_i$和$y_i$的增量形式，即原始形式可以化成：\n",
    "$$w_{t+1} = \\sum_{i=1}^{N}\\beta_i y_i x_i$$\n",
    "$$b_{t+1} = \\sum_{i=1}^N \\beta_i y_i$$\n",
    "其中，$\\beta_i = n_i \\alpha$，$n_i$表示第$i$个实例$x_i$更新的次数<br/>\n",
    "此时，模型转变为\n",
    "$$f(x) = sign(\\sum_{j=1}^N \\beta_j x_j y_j x + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T08:44:32.234948Z",
     "start_time": "2020-07-21T08:44:32.228598Z"
    }
   },
   "source": [
    "训练集包括N个样例，样本中包含n个特征，标记为二分类取值为-1或者+1\n",
    "- 输入的样例：${(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)}$，学习率：$\\alpha$\n",
    "- 输出分离超平面的系数$\\beta$, b\n",
    "\n",
    "算法执行步骤如下：\n",
    "1. 初始化$\\beta$, b以及学习率$\\alpha$\n",
    "2. 在训练集选取数据$(x_i, y_i)$\n",
    "3. 如果满足$y_i(sign(\\sum_{j=1}^N \\beta_j y_j x_j x_i + b)) <= 0$，则\n",
    "$$\\beta_j(t+1) = \\beta_j(t) + \\alpha $$\n",
    "$$b_{t+1} = b_t + \\alpha y_i$$\n",
    "4. 转至2，直到训练集中没有误分类点\n",
    "\n",
    "其中，训练实例可以通过计算Gram矩阵（即$x_i$和$x_j$的内积组成的矩阵）的形式来存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从图形中理解感知机的原始算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T09:02:44.199825Z",
     "start_time": "2020-07-21T09:02:44.196216Z"
    }
   },
   "source": [
    "为了方便说明，记$\\hat w = (w, b)$，$\\hat x = (x, 1)$，则感知机模型可以变为：\n",
    "$$f(x) = sign(\\hat w^T \\hat x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前我们说明了，只有误分类点才会对$\\hat w$进行更新。因此，考虑以下两种情况：\n",
    "1. 真实类别为y=+1, 但是模型的输出为-1\n",
    "    - 考虑到$\\hat w^T \\hat x = |\\hat w||\\hat x|cos\\theta$\n",
    "    - 对于真实类别，我们希望说明$\\hat w^T \\hat x > 0$，即$\\hat w$和$\\hat x$的夹角越小越好，而模型的输出有$\\hat w^T \\hat x < 0$，则说明$\\hat w$和$\\hat x$的夹角过大\n",
    "    - 因此，我们可以通过减少$\\hat w$和$\\hat x$的夹角来达到目的，即有$\\hat w(t+1) = \\hat w(t) + \\hat x(t)$（对应着$w_{t+1} = w_t + \\alpha y_i x_i$，且$\\alpha = 1$的情况）\n",
    "    - $y_i \\hat w_{t+1}^T \\hat x_i = y_i \\hat w_t^T \\hat x_i + y_i \\parallel \\hat x_i \\parallel \\geq y_i \\hat w_t^T \\hat x_i$\n",
    "![误分类情况1](../img/error_classify1.jpg)\n",
    "\n",
    "\n",
    "2. 真实类别为y=-1, 但是模型的输出为+1\n",
    "    - 考虑到$\\hat w^T \\hat x = |\\hat w||\\hat x|cos\\theta$\n",
    "    - 对于真实类别，我们希望说明$\\hat w^T \\hat x < 0$，即$\\hat w$和$\\hat x$的夹角越大越好，而模型的输出有$\\hat w^T \\hat x > 0$，则说明$\\hat w$和$\\hat x$的夹角过小\n",
    "    - 因此，我们可以通过增大$\\hat w$和$\\hat x$的夹角来达到目的，即有$\\hat w(t+1) = \\hat w(t) - \\hat x(t)$（对应着$w_{t+1} = w_t - \\alpha y_i x_i$，且$\\alpha$ = 1的情况）\n",
    "    - $y_i \\hat w_{t+1}^T \\hat x_i = y_i \\hat w_t^T \\hat x_i - y_i \\parallel \\hat x_i \\parallel = y_i \\hat w_t^T \\hat x_i + \\parallel \\hat x_i \\parallel \\geq y_i \\hat w_t^T \\hat x_i$\n",
    "![误分类情况2](../img/error_classify2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T10:12:33.219689Z",
     "start_time": "2020-07-21T10:12:33.213804Z"
    }
   },
   "source": [
    "其实，无论对于误分类的情况1还是情况2，总有$y_i \\hat w_{t+1}^T \\hat x_i = \\geq y_i \\hat w_t^T \\hat x_i$，因为$y_i \\hat w_t^T \\hat x_i$的符号代表是否分类正确，大小代表分类超平面是否将其“分得很开”，上面的不等式说明了，对于某个误分类点来说，更新后要比更新前要好，算法PLA对该误分类点“学习”了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机算法(PLA)的收敛性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T10:24:35.959630Z",
     "start_time": "2020-07-21T10:24:35.955412Z"
    }
   },
   "source": [
    "对于线性可分的数据集，总能找到一个或者多个分类超平面能将该数据集划分，这表明了PLA的收敛性。\n",
    "- 这部分主要参考林轩田的《机器学习基石》，个人觉得讲得要比李航的《统计学习方法》要清晰，虽然证明本质上是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明两个向量的相似性有很多方法，其中计算两个向量的内积是一种方法。当内积越大，表明两个向量越相似。当然，这需要考虑向量的长度，当模长越大时，向量的内积也会越来越大。\n",
    "- 符号说明：$w_f$代表真实的w，$w_t$代表我们找到的w，这里为了符号简洁些，不记成$\\hat w$，但是含义一样，即$w_f$和$w_t$里面包含$b$，记学习率$\\alpha = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 先讨论$w_f$和$w_t$的内积，$w_0$为0向量\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "w_f^T w_t & = w_f^T(w_{t-1} + y_ix_i) \\\\\n",
    "& = w_f^T w_{t-1} + y_i w_f^T x_i \\\\\n",
    "& \\geq w_f w_{t-1} + \\underset {i} {min} (y_i w_f^T x_i) \\\\\n",
    "& \\geq w_f w_0 + t \\underset {i} {min} (y_i w_f^T x_i) \\\\\n",
    "& = t \\underset {i} {min} (y_i w_f^T x_i)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "2. 讨论$w_f$和$w_t$的模长，由于只有误分类点才更新，所以有$y_i w_{t}^T x_i \\leq 0$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\parallel w_t \\parallel^2 & = \\parallel w_{t-1} + y_ix_i \\parallel^2 \\\\\n",
    "&= \\parallel w_{t-1} \\parallel^2 + 2y_i w_{t_1}^T x_i + \\parallel y_ix_i \\parallel^2 \\\\\n",
    "& \\leq \\parallel w_{t-1} \\parallel^2 + \\parallel x_i \\parallel^2 \\\\\n",
    "& \\leq \\parallel w_{t-1} \\parallel^2 + \\underset {i} {max} \\parallel x_i \\parallel^2 \\\\\n",
    "& \\leq \\parallel w_{0} \\parallel^2 + t \\underset {i} {max} \\parallel x_i \\parallel^2 \\\\ \n",
    "& = t \\underset {i} {max} \\parallel x_i \\parallel^2\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "3. 讨论$w_f$和$w_t$的角度\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "1 \\geq cos \\theta = \\frac{w_f^T w_t}{\\parallel w_f \\parallel \\parallel w_t \\parallel} & \n",
    "\\geq \\frac{t \\underset {i} {min} (y_i w_f^T x_i)}{\\parallel w_f \\parallel \\sqrt{t \\underset {i} {max} \\parallel x_i \\parallel^2}} \\\\\n",
    "& = \\frac{\\sqrt{t} \\underset {i} {min} (y_i w_f^T x_i)}{\\parallel w_f \\parallel \\sqrt{\\underset {i} {max} \\parallel x_i \\parallel^2}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "4. 化解得到t的关系式\n",
    "$$t \\leq \\frac{\\parallel w_f \\parallel^2 \\underset {i} {max} \\parallel x_i \\parallel^2}{\\underset {i} {min} (y_i w_f^T x_i)^2} = \\frac{R^2}{\\rho^2}$$\n",
    "其中，$$R^2 = \\underset {i} {max} \\parallel x_i \\parallel^2, \\rho = \\frac{\\underset {i} {min} (y_i w_f^T x_i)}{\\parallel w_f \\parallel}$$\n",
    "\n",
    "由上述不等式说明了，更新次数是有上限的，这也就证明了收敛性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 应用场景与缺陷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T11:22:09.481881Z",
     "start_time": "2020-07-21T11:22:09.476009Z"
    }
   },
   "source": [
    "- 感知机仅限于数据线性可分的情况，对于线性不可分的情况，该算法不收敛。\n",
    "- 感知机的收敛步数受两类别之间间隔的影响。间隔越小，收敛的步数越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从感知机的分类原理中，可以看出满足条件的超平面并不止一个，不同的超平面依赖于参数的初始值。也就是说感知机模型可以有多个解。\n",
    "- 泛化能力最好的决策超平面\n",
    "    - 能够将两个类型的样本分开\n",
    "    - 能够最大化决策边界附近的两类型之间的距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考资料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T11:15:57.613009Z",
     "start_time": "2020-07-21T11:15:57.609161Z"
    }
   },
   "source": [
    "- 李航《统计学习方法》\n",
    "- 林轩田《机器学习基石》"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
