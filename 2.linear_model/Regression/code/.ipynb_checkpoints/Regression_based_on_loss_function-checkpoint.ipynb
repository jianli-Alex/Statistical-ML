{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归模型概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;无论是统计学还是机器学习，我们最先接触的模型（统计中的参数模型，机器学习中的有监督学习）都是线性模型，一个是因为它“简单”，另一个是因为它是其他许多模型的一个衍生基础。<br/>\n",
    "&emsp;&emsp;基于历史的原因，由于以前计算资源的匮乏，手算各种复杂的模型基本是行不通的，但是基于最小二乘法的线性模型的参数是有明确表达式的，并且具有方差最小的性质。<br/>\n",
    "&emsp;&emsp;线性模型不仅仅只是一条直线，其实有很多模型都可以看做是线性的（即可以写成$Y = X\\beta + \\varepsilon$的形式），虽然它们画出来可能是一条曲线，例如像对数线性回归$log(y) = \\beta_0 + \\beta_1x$，多项式回归$y = \\beta_0 + \\beta_1x + \\beta_2x^2$，还有回归解释变量中含有哑变量（定性变量）的情况，回归样条的情况等等都可以看做是线性模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从三个层面来看线性模型 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T11:53:32.447760Z",
     "start_time": "2019-10-26T11:53:32.441764Z"
    }
   },
   "source": [
    "下面从总体，样本和数据三个层面来看一下线性模型(直接考虑多元的情况)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总体层面 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:02:06.237948Z",
     "start_time": "2019-10-26T12:02:06.231951Z"
    }
   },
   "source": [
    "从理论层面上看（无数据）有：\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_{p-1}X_{p-1} + \\varepsilon$$\n",
    "\n",
    "其中，$Y$称为响应变量或因变量，$X_i$为预测变量/解释变量/自变量，$\\varepsilon$为随机误差变量。\n",
    "\n",
    "在这里，我们可以把$(X_1, X_2, \\cdots , X_{p-1}, Y)$看做是一个总体,从理论层面上来观测整一个模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样本层面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:16:18.280689Z",
     "start_time": "2019-10-26T12:16:18.273693Z"
    }
   },
   "source": [
    "假设有$(X_{11}, X_{21}, \\cdots , X_{p-1,1}, Y_1),\\quad(X_{12}, X_{22}, \\cdots , X_{p-1,2}, Y_2),\\quad\\cdots,\\quad(X_{1n}, X_{2n}, \\cdots , X_{p-1,n}, Y_n)$等n个样本，实际上样本也是一个随机变量，那么对于第一层面，我们就可以改写成：\n",
    "\n",
    "$$\n",
    "Y = \n",
    "\\begin{pmatrix}\n",
    "    Y_1\\\\\n",
    "    Y_2\\\\\n",
    "    \\vdots\\\\\n",
    "    Y_n\n",
    "\\end{pmatrix}\n",
    ",\\quad\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "    1 & X_{11} & X_{21} & \\cdots & X_{p-1,1}\\\\\n",
    "    1 & X_{12} & X_{22} & \\cdots & X_{p-1,2}\\\\\n",
    "    \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "    1 & X_{1n} & X_{2n} & \\cdots & X_{p-1,n}\n",
    "\\end{pmatrix}\n",
    ",\\quad\n",
    "\\beta = \n",
    "\\begin{pmatrix}\n",
    "    \\beta_0\\\\\n",
    "    \\beta_1\\\\\n",
    "    \\vdots\\\\\n",
    "    \\beta_{p-1}\n",
    "\\end{pmatrix}\n",
    ",\\quad\n",
    "\\varepsilon = \n",
    "\\begin{pmatrix}\n",
    "    \\varepsilon_1\\\\\n",
    "    \\varepsilon_2\\\\\n",
    "    \\vdots\\\\\n",
    "    \\varepsilon_n\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:39:32.572776Z",
     "start_time": "2019-10-26T12:39:32.565801Z"
    }
   },
   "source": [
    "将上述模型写成矩阵的形式\n",
    "$$Y_{nx1} = X_{nxp}\\beta_{px1} + \\varepsilon_{nx1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:58:25.754702Z",
     "start_time": "2019-10-26T12:58:25.748709Z"
    }
   },
   "source": [
    "其中$X$被称为设计矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guass-Markon假设"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们知道随机变量是可以求期望，求方差的。而在统计中，线性模型是基于诸多主观假定的，其中我们常用的的假定是Guass-Markon假设。它主要假定了随机误差的期望是0，并且同方差，都是$\\sigma^2$，此外随机误差之间都是无关的。\n",
    "\n",
    "将Gauss-Markon假设写成表达式的形式：\n",
    "1. $D(\\varepsilon_i) = \\sigma^2$\n",
    "2. $cov(\\varepsilon_i) = 0$\n",
    "3. $E(\\varepsilon_i) = 0$ \n",
    "\n",
    "其中，如果将假设1和2进行合并，我们就可以将假设简化为\n",
    "1. $cov(\\varepsilon) = \\sigma^2I_n$\n",
    "2. $E(\\varepsilon_i) = 0$\n",
    "\n",
    "其中，$cov(\\varepsilon)$是指随机误差向量的协方差阵，$I_n$是$n*n$维的单位矩阵\n",
    "\n",
    "此外，我们在做线性模型的时候，通常都是假设解释变量$X_i$之间是无关的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 均值回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T13:03:19.614576Z",
     "start_time": "2019-10-26T13:03:19.608577Z"
    }
   },
   "source": [
    "当我们固定X（即相当于知道它的数值）的时候，结合Guass-Markon中$E(\\varepsilon_i) = 0$的假设，我们可以求得固定$X$下$Y$的条件期望应该为$X\\beta$，而实际上$X\\beta$是我们的预测值$\\hat y$。因此，我们也把线性回归称为均值回归.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        Y = X\\beta + \\varepsilon\\\\\n",
    "        \\\\\n",
    "        E(\\varepsilon_i) = 0\n",
    "    \\end{cases}\n",
    "    => \\quad\n",
    "    E(Y|X) = E(X\\beta + \\varepsilon|X)\n",
    "    = \n",
    "    X\\beta + E(\\varepsilon)\n",
    "    =  X\\beta\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "其中，$E(\\varepsilon)$为零向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X固定下Y的分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T13:40:36.700115Z",
     "start_time": "2019-10-26T13:40:36.694119Z"
    }
   },
   "source": [
    "实际上，因为X是固定的，$\\beta$是未知参数，也是固定的。因此, 有\n",
    "$$cov(Y) = cov(X\\beta + \\varepsilon) = cov(\\varepsilon) = \\sigma^2I_n$$\n",
    "\n",
    "说白了，就是$Y$满足一个均值为$X\\beta$,方差为$\\sigma^2I_n$的一个多维分布$F_n(X\\beta, \\sigma^2I_n)$，$Y_i$满足一个均值为$X_i\\beta$,方差为$\\sigma$的分布$F(X_i\\beta, \\sigma^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T14:09:32.989589Z",
     "start_time": "2019-10-26T14:09:32.983593Z"
    }
   },
   "source": [
    "用图来直观的表示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![固定x下y的分布图](../input/固定x下y的分布图.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而当对$\\varepsilon$加上一个正态性的假设的时候，就能够得到随机误差向量$\\varepsilon$和$Y$满足多维正态分布$\\varepsilon \\sim N(O, \\sigma^2I_n)$和$Y \\sim N(X\\beta, \\sigma^2I_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据层面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们给定观测值$(x_{11}, x_{21}, \\cdots , x_{p-1,1}, y_1),\\quad(x_{12}, x_{22}, \\cdots , x_{p-1,2}, y_2),\\quad\\cdots,\\quad(x_{1n}, x_{2n}, \\cdots , x_{p-1,n}, y_n)$，我们可以将$Y$、$X$、$\\beta$、$\\varepsilon$表示成："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T06:03:23.243785Z",
     "start_time": "2019-10-27T06:03:23.203805Z"
    }
   },
   "source": [
    "$$\n",
    "Y = \n",
    "\\begin{pmatrix}\n",
    "    y_1\\\\\n",
    "    y_2\\\\\n",
    "    \\vdots\\\\\n",
    "    y_n\n",
    "\\end{pmatrix}\n",
    ",\\quad\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "    1 & x_{11} & x_{21} & \\cdots & x_{p-1,1}\\\\\n",
    "    1 & x_{12} & x_{22} & \\cdots & x_{p-1,2}\\\\\n",
    "    \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "    1 & x_{1n} & x_{2n} & \\cdots & x_{p-1,n}\n",
    "\\end{pmatrix}\n",
    ",\\quad\n",
    "\\beta = \n",
    "\\begin{pmatrix}\n",
    "    \\beta_0\\\\\n",
    "    \\beta_1\\\\\n",
    "    \\vdots\\\\\n",
    "    \\beta_{p-1}\n",
    "\\end{pmatrix}\n",
    ",\\quad\n",
    "\\varepsilon = \n",
    "\\begin{pmatrix}\n",
    "    \\epsilon_1\\\\\n",
    "    \\epsilon_2\\\\\n",
    "    \\vdots\\\\\n",
    "    \\epsilon_n\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T06:07:12.545132Z",
     "start_time": "2019-10-27T06:07:12.537156Z"
    }
   },
   "source": [
    "此时，模型仍可以写成$Y = X\\beta + \\varepsilon$，可是可以通过实际的数值估计出$\\beta$的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T06:11:49.880078Z",
     "start_time": "2019-10-27T06:11:49.875083Z"
    }
   },
   "source": [
    "机器学习中有时也写成向量的形式$f(x) = w^Tx+b$，但其实是没有本质的区别的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T06:12:57.791173Z",
     "start_time": "2019-10-27T06:12:57.638265Z"
    }
   },
   "source": [
    "下面主要是从随机变量的层面出发来继续深入讨论线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于损失函数的估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数学和计算机领域中，通常是使用损失函数来进行参数的估计，也可以说是模型的拟合\n",
    "\n",
    "而值得注意的是，我们上述采用了的Guass-Markon假设，在参数$\\beta$的估计是不需要用到的，这只是在我们之后的检验中需要使用到。但是我们不能说参数$\\beta$的估计和我们给定的假设没有一点关系，实际上根据不同的假设，我们选定的损失函数也是不同的。这一点，在之后我会详细谈到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T06:29:32.161463Z",
     "start_time": "2019-10-27T06:29:32.153467Z"
    }
   },
   "source": [
    "先来谈一下基于损失函数估计的原理：\n",
    "1. 定义模型中第$i$个残差为：$e_i = y_i - \\hat y_i$，整个残差向量就可以写成：$e = Y - \\hat Y$，其中，$\\hat Y$是拟合结果 \n",
    "2. 主观选择一个损失函数的形式：$\\rho(e_i)$，又因为损失函数应该是包含待估参数$\\beta$，因此损失函数又可以写成是$L(\\beta)$\n",
    "3. 利用损失函数最小化得到参数的估计：$\\hat \\beta = \\underset{\\beta}{\\arg \\min} \\hspace{1mm} \\rho(e_i) = \\underset {\\beta}{\\arg \\min} \\hspace{1mm} L(\\beta)$，这一串长长的公式是说使得损失函数最小的变元$\\beta$就是我们所要的估计参数$\\hat \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二次损失 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T06:25:15.210936Z",
     "start_time": "2019-10-27T06:25:15.202944Z"
    }
   },
   "source": [
    "在做线性模型的时候，我们最常使用的是基于二次损失函数的最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义二次损失函数有：$\\sum_{i=1}^n e_i^2$ \n",
    "根据我们上面所说的均值回归，以矩阵的形式有：\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        L(\\beta) = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n(y_i - \\hat y_i)^2\\\\\n",
    "        \\\\\n",
    "        \\hat Y = X\\beta\n",
    "    \\end{cases}\n",
    "    => \\sum_{i=1}^n (y_i - X_i\\beta)^2 = (Y - X\\beta)^T(Y - X\\beta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是，下文中有提到$\\hat Y = X\\hat \\beta$，严格来说上面提到的$\\hat Y$是真实的拟合值，而下文提到的$\\hat Y$是预测的拟合值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T07:21:27.691525Z",
     "start_time": "2019-10-27T07:21:27.684533Z"
    }
   },
   "source": [
    "进一步化简有（其中，$Y^TX\\beta$是一个数）：\n",
    "$$(Y - X\\beta)^T(Y - X\\beta) = Y^TY - 2Y^TX\\beta + \\beta^TX^TX\\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T07:27:20.110947Z",
     "start_time": "2019-10-27T07:27:20.103950Z"
    }
   },
   "source": [
    "要使损失函数最小化就是对损失函数求导，取到它的最小值（而这个损失函数是一个凸函数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数是最小的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T07:29:24.409036Z",
     "start_time": "2019-10-27T07:29:24.402039Z"
    }
   },
   "source": [
    "在求导之前，我们先思考一个问题，为什么$\\sum_{i=1}^n (y_i - X_i\\beta)^2$是最小的（即为什么不选择其他$\\sum_{i=1}^n (y_i - ?)^2$，为什么前面的要小于等于$\\sum_{i=1}^n (y_i - ?)^2$）\n",
    "\n",
    "实际上，这就相当于对于$E(X - ?)^2$来说，当`?`取什么时，这个式子达到最小。而我们知道，当`?`是$X$的期望$E(X)$，这个式子就能达到最小。同理（求和和期望只是差了一个系数），对于$\\sum_{i=1}^n (y_i - ?)^2$）来说，当`?`是$Y_i$的期望$E(Y_i)$，这个式子就能达到最小，而这个$E(Y_i)$恰好是$X_i\\beta$，也就是$\\hat Y_i$实际上是这组数据中，$Y$的均值。这样也就保证我们选择的损失函数是在这种二次形式中是最好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数最小化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T07:58:25.156438Z",
     "start_time": "2019-10-27T07:58:25.145424Z"
    }
   },
   "source": [
    "下面对损失函数进行求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\frac {\\mathrm{d}L(\\beta)}{\\mathrm{d}\\beta} & = \\frac {\\mathrm{d} Y^TY - 2Y^TX\\beta + \\beta^TX^TX\\beta}{\\mathrm{d} \\beta}\\\\\n",
    "        & = 0 - 2X^TY + 2X^TX\\beta\n",
    "    \\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T08:32:31.469316Z",
     "start_time": "2019-10-27T08:32:31.460325Z"
    }
   },
   "source": [
    "令$\\frac {\\mathrm{d}L(\\beta)}{\\mathrm{d}\\beta} = 0$，有：\n",
    "\\begin{equation}\n",
    "    0 - 2X^TY + 2X^TX\\beta = 0 => \\hat \\beta = \n",
    "    \\begin{cases}\n",
    "        (X^TX)^{-1}X^TY, & (X^TX)可逆 \\\\\n",
    "        \\\\\n",
    "        (X^TX)^-X^TY, & (X^TX)不可逆\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如之前提到的，我们通常假定解释变量$X_i$是无关的（即列满秩），此时便有$(X^TX)$可逆(因为$Ax=0$和$A^TAx=0$是同解方程组，所以可以证出$r(A)=r(A^TA)$的)，所以通常将待估参数表示为$\\hat \\beta = (X^TX)^{-1}X^TY$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T08:42:12.019227Z",
     "start_time": "2019-10-27T08:42:12.011234Z"
    }
   },
   "source": [
    "从上述的步骤中，我们是可以看到做估计的时候没有用到Guass-Markon的假设。而在数学和机器学习中，有时也就直接使用参数估计后的模型进行测试，然后求出它的均方误差，并没有对这个估计量做检验。而实际上，$\\hat \\beta$是**最佳的线性无偏估计**，说它最佳是因为它方差是最小的（[检验估计量的有效性中将会提到](#方差最小)）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后值得一提的是，上面所述的二次损失形式其实是基于方差齐性$cov(\\varepsilon)=\\sigma^2I_n$的假设，当我们改变这个假设的时候，二次损失的将会有所不同，这个我之后会进一步提到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T12:35:33.313344Z",
     "start_time": "2019-10-27T12:35:33.306349Z"
    }
   },
   "source": [
    "既然提到了二次损失，在这里也提一下其他的一些损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最小绝对损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T12:37:41.925575Z",
     "start_time": "2019-10-27T12:37:41.917584Z"
    }
   },
   "source": [
    "定义为：$L(\\beta) = \\sum_{i=1}^n |y_i - x_i\\beta|$\n",
    "\n",
    "此外，对于$E(|y_i - ?|)$，当？取中位数的时候，这个损失函数是最小的，所以有时也称为最小中位估计。而由于中位数比平均数要稳健，所以对比二次损失，绝对损失对异常点不敏感。\n",
    "\n",
    "当然这里，当我们假定$\\varepsilon$是服从均值为0的正态分布的时候，$X\\beta$实际上也是固定X下Y的中位数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分位回归的损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T13:01:16.515691Z",
     "start_time": "2019-10-27T13:01:16.507696Z"
    }
   },
   "source": [
    "定义为：$L(\\beta) = \\sum_{i=1}^n e_i(\\tau - I(e_i < 0))$，其中I()是指示函数，$0 < \\tau < 1$。\n",
    "\n",
    "上面所述的都是对称的损失函数，但是分位回归的损失函数是不对称的，实际上不对称带来的影响会左右曲线的拟合，这时就需要使用分位回归的损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T13:18:20.623377Z",
     "start_time": "2019-10-27T13:18:20.616382Z"
    }
   },
   "source": [
    "正如下面这个图所示：\n",
    "![分位回归的例子](../input/分位回归的例子.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T13:20:47.716674Z",
     "start_time": "2019-10-27T13:20:47.709678Z"
    }
   },
   "source": [
    "实际上，绝对损失可以看做是分位回归的损失函数的一个特例，当$\\tau$取0.5时，两者实际上就差了一个常数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T13:22:49.981239Z",
     "start_time": "2019-10-27T13:22:49.975245Z"
    }
   },
   "source": [
    "当然除了上面几个损失函数外，还有像用于稳健回归的huber函数等。实际上，绝对损失也是huber函数的一种特例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 估计量$\\hat \\beta$的检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T12:27:32.564216Z",
     "start_time": "2019-10-27T12:27:32.552224Z"
    }
   },
   "source": [
    "下面我们来谈谈为什么这个估计是最佳线性无偏估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 估计量的评价标准"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "估计量的评价标准主要有三个，无偏性、有效性、一致性。以这里的$\\hat \\beta$为例，\n",
    "- 无偏性指的是估计量的期望等于参数的真实值，即$E(\\hat \\beta) = \\beta$，其中$^\\beta$实际上是一个随机向量，因为X是固定的，而Y是随机向量。\n",
    "- 有效性是指在诸多无偏估计中，方差最小的那一个估计量。\n",
    "- 一致性是描述当样本容量无穷时，估计量限接近参数的真实值，即$n-> + \\infty, \\hat \\beta -> \\beta$，表示成数学语言就是$P_{n-> + \\infty}(|\\hat \\beta - \\beta| < \\varepsilon) = 1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T09:48:40.857008Z",
     "start_time": "2019-10-27T09:48:40.850013Z"
    }
   },
   "source": [
    "在这之中，一致性是最容易满足的，其次是无偏性，最后才是有效性。在这里，只讨论无偏性和有效性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 无偏性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T09:24:31.844594Z",
     "start_time": "2019-10-27T09:24:31.833583Z"
    }
   },
   "source": [
    "讨论这个估计是否是无偏估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T09:26:46.361507Z",
     "start_time": "2019-10-27T09:26:46.353518Z"
    }
   },
   "source": [
    "$$E(\\hat \\beta) = E((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^TE(Y) = (X^TX)^{-1}X^TX\\beta = \\beta$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有效性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T10:54:21.688734Z",
     "start_time": "2019-10-27T10:54:21.683738Z"
    }
   },
   "source": [
    "经过上面的证明，我们可以得到：\n",
    "$$\n",
    "    \\hat Y = X \\hat \\beta = X(X^TX)^{-1}X^TY = HY\n",
    "$$\n",
    "其中，记$H = X(X^TX)^{-1}X^T$，且称$H$为帽子矩阵，这个帽子矩阵有优良的性质，是一个幂等对称矩阵。即有$H^T = H$、$H^2 = H$。此外，也把$H$称为投影矩阵，即可以将随机向量$Y$投影到$X$的超平面上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T12:23:57.621236Z",
     "start_time": "2019-10-27T12:23:57.614238Z"
    }
   },
   "source": [
    "再来看一副图<a id=\"方差最小\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T12:24:29.926662Z",
     "start_time": "2019-10-27T12:24:29.836713Z"
    }
   },
   "source": [
    "![Y在X平面的投影](../input/Y在X平面的投影.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道当垂直时，线段到平面的距离是最短的。即当$HY\\perp(I_n - H)Y$时，残差$e$是最小的，这也直观地证明这个估计是方差最小的（因为残差$e$是方差的一个度量），因此我们也称这个估计$\\hat \\beta$是**一切线性无偏估计中方差最小**的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T10:59:41.826468Z",
     "start_time": "2019-10-27T10:59:41.818475Z"
    }
   },
   "source": [
    "既然$\\hat \\beta$是最小的，那么下面来推导下它的方差究竟是多少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T10:45:52.130298Z",
     "start_time": "2019-10-27T10:45:52.121303Z"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        cov(\\hat \\beta) & = cov((X^TX)^{-1}X^TY)\\\\\n",
    "        & = (X^TX)^{-1}X^Tcov(Y)X(X^TX)^{-1}\\\\\n",
    "        & = (X^TX)^{-1}X^T\\sigma^2I_nX(X^TX)^{-1}\\\\\n",
    "        & = \\sigma^2(X^TX)^{-1}\n",
    "    \\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T11:01:37.841581Z",
     "start_time": "2019-10-27T11:01:37.834590Z"
    }
   },
   "source": [
    "因此，根据$\\hat \\beta_i$的协方差，我们可以得到$\\hat \\beta_i$的标准误差为：\n",
    "$$st.dev(\\hat \\beta_i) = \\sigma \\sqrt{(X^TX)^{-1}_{ii}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T11:16:36.088718Z",
     "start_time": "2019-10-27T11:16:36.081722Z"
    }
   },
   "source": [
    "在这里，我们终于见到了假设中的\\sigma，而这是实际上是要在后面的假设检验中需要用到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\sigma^2$的估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T11:19:06.831681Z",
     "start_time": "2019-10-27T11:19:06.824685Z"
    }
   },
   "source": [
    "虽然算出了$\\hat \\beta_i$的标准误差，但是它表达式中的$\\sigma$实际上是一个未知参数，下面对其进行估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道方差是指偏离平均水平的程度，因此我们很自然地想到使用$\\frac{\\displaystyle \\sum_{i=1}^n(y_i - \\hat y_i)^2}{n}$来估计，即使用$\\frac{\\displaystyle \\sum_{i=1}^n(e_i)^2}{n}$来估计，因为我们之前说过$\\hat Y$是X固定下Y的平均水平。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T11:31:18.795970Z",
     "start_time": "2019-10-27T11:31:18.783980Z"
    }
   },
   "source": [
    "因此，有\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\displaystyle \\sum_{i=1}^n(e_i)^2 & = e^Te\\\\\n",
    "        & = Y^T(I_n - H)^T(I_n - H)Y \\\\\n",
    "        & = Y^T(I_n - H)Y  \n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "其中，由于$H$是幂等对称阵，所以$(I_n - H)$实际上也是幂等对称阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "证法1："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T11:50:21.917135Z",
     "start_time": "2019-10-27T11:50:21.908122Z"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        E(Y^T(I_n - H)Y) & = E(tr(Y^T(I_n - H)Y))\\\\\n",
    "        & = E[tr(YY^T(I_n - H))]\\\\\n",
    "        & = tr[(I_n - H)E(YY^T)]\\\\\n",
    "        & = tr(I_n - H)[cov(Y) + E(Y)E(Y)^T]\\\\\n",
    "        & = tr(I_n - H)[\\sigma^2I_n + X\\beta\\beta^TXT]\\\\\n",
    "        & = (n-p)\\sigma^2 + tr[(I_n - H)X\\beta\\beta^TXT]\\\\\n",
    "        & = (n-p)\\sigma^2 + tr[(I_n - X(X^TX)^{-1}X^T)X\\beta\\beta^TXT]\\\\\n",
    "        & = (n-p)\\sigma^2\n",
    "    \\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T12:02:25.033762Z",
     "start_time": "2019-10-27T12:02:25.027765Z"
    }
   },
   "source": [
    "证法2："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        E(Y^T(I_n - H)Y) & = E(tr(Y^T(I_n - H)Y))\\\\\n",
    "        & = E(Y^T)(I_n - H)E(Y) + tr((I_n - H)cov(Y))\\\\\n",
    "        & = \\beta^TX^T(I_n - H)X\\beta + (n-p)\\sigma^2\\\\\n",
    "        & = \\beta^TX^T(I_n - X(X^TX)^{-1}X^T)X\\beta + (n-p)\\sigma^2\\\\\n",
    "        & = (n-p)\\sigma^2\n",
    "    \\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T12:05:11.518564Z",
     "start_time": "2019-10-27T12:05:11.512566Z"
    }
   },
   "source": [
    "其中，由于帽子矩阵的是幂等阵，所以特征值只有0或者1，设有p个特征值为1，因此$I_n - H$有n-p个特征值为1，即$tr(I_n - H) = n-p$\n",
    "\n",
    "实际上，当X列满秩的时候，有$tr(H) = tr(X(X^TX)^{-1}X^T) = tr(X^TX(X^TX)^{-1}) = tr(I_p) = p$，因此可以看出H的秩等于$X$的维数，也等于$X$的变量个数加上常数。\n",
    "\n",
    "在这里，我们也更加清楚，为什么我们要假设解释变量$X_i$都是无关的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T13:28:19.768475Z",
     "start_time": "2019-10-27T13:28:19.761459Z"
    }
   },
   "source": [
    "由上面的推导过程可以看出，$\\sigma^2$的无偏估计应该是$\\frac {\\displaystyle \\sum_{i=1}^n e_i}{n-p}$，将残差平方和$\\displaystyle \\sum_{i=1}^n e_i$记为$SSE$，并且记均方误差为$MSE$，即有：\n",
    "\n",
    "$$\\hat \\sigma^2 = \\frac {SSE}{n-p} = MSE$$,其中$p$为变量个数加上常数个数（即设计矩阵$X$的维数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 估计量的区间估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T13:24:41.445543Z",
     "start_time": "2019-10-27T13:24:41.439546Z"
    }
   },
   "source": [
    "上面是做的估计都是点估计，下面谈谈怎么做区间估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T13:39:25.053183Z",
     "start_time": "2019-10-27T13:39:25.047205Z"
    }
   },
   "source": [
    "我们的目标是找到$\\bar \\beta$和$\\underline \\beta$，使得\n",
    "$$P(\\bar \\beta_i < \\beta_i < \\underline \\beta_i) = 1 - \\alpha$$\n",
    "\n",
    "为了实现这一目标，我们来构造含未知参数$\\beta$的枢轴量\n",
    "\n",
    "对于$\\beta_i$，我们可以对它进行标准化，有：\n",
    "$$\\frac {\\hat \\beta_i - \\beta_i}{\\sigma \\sqrt{(X^TX)^{-1}_{ii}}} \\sim N(0, 1) \\tag{1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T13:51:05.122218Z",
     "start_time": "2019-10-27T13:51:05.117218Z"
    }
   },
   "source": [
    "此外，我们考虑$\\frac{\\hat \\sigma}{\\sigma}$\n",
    "$$\n",
    "    \\frac{\\hat \\sigma}{\\sigma} = \\sqrt{\\frac {\\frac {\\displaystyle \\sum_{i=1}^n (y_i - \\hat y_i)^2}{n-p}}{\\sigma^2}}\n",
    "     = \\sqrt{\\frac {\\displaystyle \\sum_{i=1}^n (\\frac {y_i - \\hat y_i}{\\sigma})^2}{n-p}} \n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "其中，$\\displaystyle \\sum_{i=1}^n (\\frac {y_i - \\hat y_i}{\\sigma})^2 \\sim \\chi^2(n-p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述(1)和(2)结合起来，有\n",
    "$$\n",
    "\\frac {\\hat \\beta_i - \\beta_i}{\\hat \\sigma \\sqrt{(X^TX)^{-1}_{ii}}} = \\frac{\\frac {\\hat \\beta_i - \\beta_i}{\\sigma \\sqrt{(X^TX)^{-1}_{ii}}}} {\\frac {\\hat \\sigma}{\\sigma}} = \\frac{\\frac {\\hat \\beta_i - \\beta_i}{\\sigma \\sqrt{(X^TX)^{-1}_{ii}}}} {\\sqrt{\\frac {\\displaystyle \\sum_{i=1}^n (\\frac {y_i - \\hat y_i}{\\sigma})^2}{n-p}}} \\sim t(n-p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T14:14:47.353244Z",
     "start_time": "2019-10-27T14:14:47.345247Z"
    }
   },
   "source": [
    "即有区间估计，$\\beta_i \\underline + t_{1 - \\frac{\\alpha}{2}}(n-p) \\hat{st.dev}(\\hat \\beta)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归均值$\\hat y$的区间估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 置信区间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从直观来理解，$\\hat Y$是一组样本数据的Y的期望，但是当样本数据不同时，这个期望必然会发生一定的变化，那么这个变化的区间就是我们要估计的区间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T06:03:12.875985Z",
     "start_time": "2019-10-28T06:03:12.868988Z"
    }
   },
   "source": [
    "从数理的角度来看，由于估计量$\\hat \\beta = (X^TX)^{-1}X^TY$，$Y$是随机的，即估计量$\\hat \\beta$也是一个随机的。并且由于$\\hat Y_i = X_i \\hat \\beta$(这也是$\\hat Y_i$的点估计)，所以$\\hat Y_i$也是随机的。这样就可以对$\\hat Y_i$做区间估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T06:31:59.736570Z",
     "start_time": "2019-10-28T06:31:59.730573Z"
    }
   },
   "source": [
    "在构造枢轴量之前，我们先做一些准备工作\n",
    "$$cov(\\hat Y) = cov(X\\hat \\beta) = Xcov(\\hat \\beta)X^T = X \\sigma^2 (X^TX)^{-1}X^T$$\n",
    "\n",
    "即有：\n",
    "$$st.dev(\\hat Y_i) = st.dev(X_i \\hat \\beta) = \\sigma \\sqrt{X_i(X^TX)^{-1}X_i^T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T06:15:38.899495Z",
     "start_time": "2019-10-28T06:15:38.890504Z"
    }
   },
   "source": [
    "构造枢轴量：\n",
    "$$\n",
    "\\frac {\\hat Y_i - X_i\\beta}{\\hat \\sigma \\sqrt{X_i(X^TX)^{-1}X_i^T}} = \\frac {\\frac {\\hat Y_i - X_i \\beta}{\\sigma \\sqrt{X_i(X^TX)^{-1}X_i^T}}}{\\frac{\\hat \\sigma}{\\sigma}} = \\frac {\\frac {\\hat Y_i - X_i \\beta}{\\sigma \\sqrt{X_i(X^TX)^{-1}X_i^T}}}{\\sqrt{\\frac {\\displaystyle \\sum_{i=1}^n (\\frac {y_i - \\hat y_i}{\\sigma})^2}{n-p}}} \\sim t(n-p)\n",
    "$$\n",
    "\n",
    "即有置信区间估计为：$\\hat Y_i \\underline + t_{1 - \\frac{\\alpha}{2}}(n-p) \\hat{st.dev}(\\hat Y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测区间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测区间其实是对新的观测值的一个估计区间，可以将(X)，即有$Y_i = \\hat Y_i + \\varepsilon_i = X_i\\hat \\beta + \\varepsilon$，后两者是随机变量，所以$Y_i$也是随机变量\n",
    "\n",
    "所以它的方差应该为\n",
    "$$\n",
    "var(\\hat y_0 - y_0) = var(\\hat Y_i + \\varepsilon_i) = var(\\hat Y_i) + var(\\varepsilon_i) = \\hat \\sigma^2 X_i(X^TX)^{-1}X_i^T + \\hat \\sigma^2 = \\hat \\sigma^2(1 + X_i(X^TX)^{-1}X_i^T)\n",
    "$$\n",
    "\n",
    "即有标准误差为：\n",
    "$$\n",
    "st.dev(Y_i) = \\sigma^2 \\sqrt{1 + X_i(X^TX)^{-1}X_i^T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "构造枢轴量：\n",
    "$$\n",
    "\\hat Y_0\n",
    "$$\n",
    "\n",
    "即有区间估计：\n",
    "$$\n",
    "\\hat Y \\underline + t_{1- \\sqrt{\\alpha}{2}}\\hat{st.dev}(Y_i)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
